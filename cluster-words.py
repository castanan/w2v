# Jorge Castanon, October 2015
# Data Scientist @ IBM

# run in terminal with:
# ~/Documents/spark-1.5.1/bin/spark-submit cluster-words.py
# Replace this line with:
# /YOUR-SPARK-HOME/bin/spark-submit cluster-words.py

import numpy as np
import math

from pyspark.context import SparkContext
from pyspark.mllib.clustering import KMeans

# next 2 lines can be reaplced to read from hdfs, 
# if the Word2Vec matrix is big 
Feat = np.load('myW2Vmatrix.npy')    # reads model generated by Word2Vec
words = np.load('myWordList.npy')    # reads list of words

print "\n================================================="
print "Size of the Word2Vec matrix is: ", Feat.shape 
print "Number of words in the models: ", words.shape
print "=================================================\n"

## Spark Context
sc = SparkContext('local','cluster-words') 
# next 3 lines turn some logs off
logger = sc._jvm.org.apache.log4j
logger.LogManager.getLogger("org").setLevel( logger.Level.OFF )
logger.LogManager.getLogger("akka").setLevel( logger.Level.OFF )

## Read the Word2Vec model
# the next line should be read/stored from hdfs if it is large
Feat = sc.parallelize(Feat) 

## K-means clustering with Spark
K = int(math.floor(math.sqrt(float(words.shape[0])/2))) # Number of clusters
         # K ~ sqrt(n/2) this is a rule of thumb for choosing K,
         # where n is the number of words in the model
         # feel free to choose K with a fancier algorithm
maxiters = 200 # may change depending on the data        
clusters = KMeans.train(Feat, k = K, maxIterations = maxiters) 

print "\n================================================="
print "Number of clusters used: ", K
print "=================================================\n"

## Getting Cluster Labels for each Word and saving to a numpy file
labels =  Feat.map(lambda point: clusters.predict(point)) # add labels to each vector (word)
list_labels = labels.collect()
np.save('myClusters.npy',list_labels)

sc.stop()

